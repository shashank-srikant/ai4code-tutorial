{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/shashank-srikant/ai4code-tutorial\n",
    "!cd ai4code-tutorial\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_tokens_to_use(tokenizer, filename=\"vocab_tokens_to_use.pkl\"):\n",
    "\tdef isascii(s, ignore=['*', '@', '[', '\\\\', ']', '/', '<', '=', '>', '^', '_', '`', '{', '}', '|','~']):\n",
    "\t\t\"\"\"Check if the characters in string s are in ASCII, U+0-U+7F.\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\ts_enc = s.encode('ascii')\n",
    "\t\t\treturn (not any(i in s for i in ignore))\n",
    "\t\texcept:\n",
    "\t\t\treturn False\n",
    "\t\n",
    "\tif os.path.exists(filename):\n",
    "\t\twith open(filename, 'rb') as f:\n",
    "\t\t\t(vocab_tokens_to_use, vocab_tokens_to_ignore, vocab_tokens_not_upper_case, vocab_tokens_upper_case) = pickle.load(f)\n",
    "\telse:\n",
    "\t\tall_tokens = tokenizer.get_vocab()\n",
    "\t\tcntr, alt_tok = 0, {}\n",
    "\t\ttoks_to_use, toks_to_ignore = [], []\n",
    "\t\ttoks_lower_case, toks_upper_case, toks_other_case = [], [], []\n",
    "\t\t\n",
    "\t\tfor k, v in all_tokens.items():\n",
    "\t\t\tif isascii(k):\n",
    "\t\t\t\talt_tok[k] = v\n",
    "\t\t\t\tcntr += 1\n",
    "\t\t\t\ttoks_to_use.append(v)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif k[0].isupper() and k[0] != 'Ä ':\n",
    "\t\t\t\t\ttoks_upper_case.append(v)\n",
    "\t\t\t\telif k[0].islower():\n",
    "\t\t\t\t\ttoks_lower_case.append(v)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttoks_other_case.append(v)\n",
    "\t\t\telse:\n",
    "\t\t\t\ttoks_to_ignore.append(v)\n",
    "\n",
    "\t\tvocab_tokens_to_ignore = sorted(toks_to_ignore)\n",
    "\t\tvocab_tokens_to_use = sorted(toks_to_use)\n",
    "\t\tvocab_tokens_not_upper_case = sorted(toks_lower_case + toks_other_case)\n",
    "\t\tvocab_tokens_upper_case = sorted(toks_upper_case)\n",
    "\t\twith open(filename, 'wb') as f:\n",
    "\t\t\tpickle.dump((vocab_tokens_to_use, vocab_tokens_to_ignore, vocab_tokens_not_upper_case, vocab_tokens_upper_case), f)\n",
    "\t\n",
    "\treturn vocab_tokens_to_use, vocab_tokens_to_ignore, vocab_tokens_not_upper_case, vocab_tokens_upper_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions, \n",
    "    BaseModelOutputWithPoolingAndCrossAttentions, \n",
    "    SequenceClassifierOutput\n",
    ")\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertEmbeddings,\n",
    "    BertModel\n",
    ")\n",
    "\n",
    "class CustomBertEmbeddings(BertEmbeddings):\n",
    "    def __init__(self, config, vocab_size, embed_dim):\n",
    "        super().__init__(config)\n",
    "        self.word_embeddings_v2 = torch.nn.Linear(vocab_size, embed_dim, bias=False)\n",
    "\n",
    "    def update_weights(self):\n",
    "        assert self.word_embeddings_v2.weight.data.shape == torch.t(self.word_embeddings.weight.data).shape\n",
    "        self.word_embeddings_v2.weight.data = torch.t(self.word_embeddings.weight.data)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None, \n",
    "        token_type_ids=None, \n",
    "        position_ids=None, \n",
    "        inputs_embeds=None, \n",
    "        past_key_values_length=0,\n",
    "        one_hot=None\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            if one_hot is not None:\n",
    "                inputs_embeds = self.word_embeddings_v2(one_hot)\n",
    "            else:\n",
    "                inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class CustomBertModel(BertModel):\n",
    "    def __init__(self, config, vocab_sz, hidden_sz):\n",
    "        super().__init__(config)\n",
    "        self.embeddings_v2 = CustomBertEmbeddings(config, vocab_sz, hidden_sz)\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.embeddings_v2.load_state_dict(self.embeddings.state_dict(), strict=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        one_hot=None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings_v2.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings_v2(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "            one_hot=one_hot\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "class CustomBertForSequenceClassification(transformers.BertForSequenceClassification):\n",
    "    def __init__(self, config, vocab_sz, hidden_sz):\n",
    "        super().__init__(config)\n",
    "        self.bert_v2 = CustomBertModel(config, vocab_sz, hidden_sz)\n",
    "    \n",
    "    def update_weights(self):\n",
    "        self.bert_v2.load_state_dict(self.bert.state_dict(), strict=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        one_hot=None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert_v2(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            one_hot=one_hot\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_sensitive_sites(model, code, target, inp_oh, number_of_sites, get_toks_per_word, tokenizer_for_debugging_purposes):\n",
    "        # Use word importance function I(x_i) from https://arxiv.org/pdf/2109.00544.pdf\n",
    "        grads_and_embeddings = model.get_grad_v2(code, target, None, inp_oh)\n",
    "        g = grads_and_embeddings['gradient'].detach().data\n",
    "        abs_g = torch.abs(g)\n",
    "        sum_g = torch.sum(abs_g, dim=1)\n",
    "        sorted_sum_g = torch.sort(sum_g, descending=True).indices.numpy().tolist()\n",
    "\n",
    "        tok_idxss = grads_and_embeddings['ids']\n",
    "        word_idxs = get_toks_per_word(tok_idxss)\n",
    "        words_to_replace = []\n",
    "        for k in word_idxs:\n",
    "                if len(k) == 1:\n",
    "                        words_to_replace.append(k[0])\n",
    "\n",
    "        num_replace, idxs_to_replace = 0, []\n",
    "        for s in sorted_sum_g:\n",
    "                if (s in words_to_replace) and (s != (len(sorted_sum_g) - 1)):\n",
    "                        num_replace += 1\n",
    "                        idxs_to_replace.append(s)\n",
    "\n",
    "                if num_replace == number_of_sites:\n",
    "                        break\n",
    "\n",
    "        return idxs_to_replace\n",
    "\n",
    "\n",
    "def get_code_preds(sent, model, tokenizer, target_output=None, attack_loss_fn=None, input_oh=None):\n",
    "    encoded_input = tokenizer(sent, return_tensors='pt')\n",
    "    output = model(**encoded_input, one_hot=input_oh)\n",
    "    if target_output is not None:\n",
    "        loss = attack_loss_fn(output.logits, torch.tensor(target_output).unsqueeze(0)).detach().cpu().numpy().tolist()\n",
    "    else:\n",
    "        loss = None\n",
    "    return output.logits.detach().cpu().squeeze(), encoded_input['input_ids'], encoded_input, loss\n",
    "\n",
    "\n",
    "def per_code_synthesis_senti(code_to_transform, \n",
    "                                tokenizer, \n",
    "                                config,\n",
    "                                model_wrapper_senti, \n",
    "                                custom_bert, \n",
    "                                desired_target_after_attack,\n",
    "                                args, \n",
    "                                convert_to_onehot,\n",
    "                                get_toks_per_word,\n",
    "                                attack_iters,\n",
    "                                learning_rate,\n",
    "                                vocab_tokens_to_use_bert, \n",
    "                                number_to_dump,\n",
    "                                paths,\n",
    "                                model1=None, model2=None):\n",
    "\n",
    "    # code_to_transform = \"I am very sad today.\"\n",
    "    \n",
    "    result_dump = []\n",
    "    result_dump.insert(0, ['pgd_iters: '+str(attack_iters), \n",
    "                    'pgd_lr: '+str(learning_rate), \n",
    "                    'desired_target: '+str(desired_target_after_attack), \n",
    "                    'number of multinomial samples: '+str(args.pgd_multinomial_samples),\n",
    "                    'number sites attacked: '+str(args.pgd_number_sites_to_attack),\n",
    "                    'custom input file: '+str(args.pgd_load_csv_pth),\n",
    "                    'code idx '+str(args.pgd_load_csv_idx),\n",
    "                    '', '', '', ''])\n",
    "\n",
    "    result_dump.insert(1, ['Orig code', \n",
    "                    'Orig prediction',\n",
    "                    'Orig prediction loss', \n",
    "                    'Generated sent', \n",
    "                    'Generated prediction',\n",
    "                    'Generated prediction loss', \n",
    "                    'Best loss iters',\n",
    "                    'Number of sites selected for perturbation',\n",
    "                    'Number of sites finally pertrubed',\n",
    "                    'Selected sites for perturbation',\n",
    "                    'Processing time',\n",
    "                    'CSV Idx'])\n",
    "    \n",
    "    df = pd.DataFrame(result_dump)\n",
    "    identifier_dir = str(number_to_dump)+\"_\"+\\\n",
    "                str(attack_iters)+\"_\"+\\\n",
    "                str(learning_rate)+\"_\"+\\\n",
    "                str(desired_target_after_attack)+\"_\"+\\\n",
    "                str(args.pgd_multinomial_samples)+\"_\"+\\\n",
    "                str(args.pgd_number_sites_to_attack)+\"_\"+\\\n",
    "                str(args.pgd_min_sites_to_attack)+\"_\"+\\\n",
    "                str(args.pgd_max_sites_to_attack)+\"_\"+\\\n",
    "                str(args.pgd_load_csv_pth.split(\"/\")[-1].split(\".\")[0])\n",
    "    \n",
    "    if args.pgd_number_sites_to_attack == -1:\n",
    "        number_sites_to_attack = np.random.randint(args.pgd_min_sites_to_attack, args.pgd_max_sites_to_attack)\n",
    "    else:\n",
    "        number_sites_to_attack = args.pgd_number_sites_to_attack\n",
    "\n",
    "    (prediction_orig, tok_idxs, encoded_idxs, loss_orig) = get_code_preds(code_to_transform, model_wrapper_senti.model, tokenizer, desired_target_after_attack, model_wrapper_senti.attack_loss_fn, None)\n",
    "    # print(\"Original code: {}; Predicted activation: {}\\n^^^^^\\n\".format(code_to_transform, prediction))\n",
    "\n",
    "    input_onehot = convert_to_onehot(tok_idxs, vocab_size=len(tokenizer), device=args.device)\n",
    "    input_onehot_orig = input_onehot.detach().clone()\n",
    "\n",
    "    loss_iters, generated_tokenizer_idxs = [], None\n",
    "    input_onehot_best = None\n",
    "    loss_best = 100 #loss_prediction\n",
    "    pred_best = 0\n",
    "    \n",
    "    ## Test whether onehot preds work as expected\n",
    "    input_onehot.grad = None\n",
    "    input_onehot.requires_grad = True\n",
    "    input_onehot.retain_grad()\n",
    "    (prediction_oh, _, _, _) = get_code_preds(code_to_transform, model_wrapper_senti.model, tokenizer, None, None, input_onehot)\n",
    "    assert torch.equal(prediction_oh, prediction_orig)\n",
    "\n",
    "    tok_to_attack = get_most_sensitive_sites(model_wrapper_senti, code_to_transform, desired_target_after_attack, input_onehot, number_sites_to_attack, get_toks_per_word, tokenizer)\n",
    "    input_onehot.requires_grad = False\t\t\t\n",
    "\n",
    "    input_onehot_softmax = input_onehot.data.clone()\n",
    "    # for attack_cnt in tqdm(range(pgd_attack_iters)):\n",
    "    for attack_cnt in range(attack_iters):\n",
    "        loss_best_sampled = 100 #loss_prediction\n",
    "        pred_best_sampled = 0\n",
    "        best_input_onehot_sampled, best_nabla_sampled = None, None\n",
    "\n",
    "        if attack_cnt % 5 == 0:\n",
    "                flg = True\n",
    "        else:\n",
    "                flg = False\n",
    "\n",
    "        for _ in range(args.pgd_multinomial_samples):\n",
    "            input_onehot_softmax_ = input_onehot_softmax.data.numpy()[0,:]\n",
    "            sampled_oh_ = []\n",
    "            for tok_idx in range(input_onehot_softmax_.shape[0]):\n",
    "                if tok_idx in tok_to_attack:\n",
    "                    sampled_oh_.append(np.random.multinomial(1, input_onehot_softmax_[tok_idx]))\n",
    "                else:\n",
    "                    sampled_oh_.append(input_onehot.data[:, tok_idx, :].squeeze(0).numpy())\n",
    "            sampled_oh = np.stack(sampled_oh_)\n",
    "            input_onehot_softmax_sampled = torch.tensor(sampled_oh, requires_grad=True, dtype=torch.double, device=args.device)\n",
    "            grads_and_embeddings = model_wrapper_senti.get_grad_v2(code_to_transform, desired_target_after_attack, None, input_onehot_softmax_sampled, False)\n",
    "            if (grads_and_embeddings['loss'] < loss_best_sampled): # or\\\n",
    "                # (desired_target < 0 and grads_and_embeddings['loss'] > loss_best_sampled):\n",
    "                loss_best_sampled = grads_and_embeddings['loss']\n",
    "                pred_best_sampled = grads_and_embeddings['prediction']\n",
    "                #if args.verbose:\n",
    "                #\tprint(\"Loss: {}; Pred: {}\".format(loss_best_sampled, grads_and_embeddings['prediction']))\n",
    "                best_input_onehot_sampled = input_onehot_softmax_sampled.detach().clone()\n",
    "                best_nabla_sampled = grads_and_embeddings['gradient'].detach()\n",
    "                \n",
    "        loss_iters.append(loss_best_sampled)\n",
    "\n",
    "        if (loss_best_sampled < loss_best): # or (desired_target < 0 and loss_best_sampled > loss_best):\n",
    "            loss_best = loss_best_sampled\n",
    "            pred_best = pred_best_sampled\n",
    "            input_onehot_best = best_input_onehot_sampled.data\n",
    "            if args.verbose:\n",
    "                print(\"Loss: {}; Pred: {}; iter: {}\".format(loss_best, pred_best, attack_cnt))\n",
    "\n",
    "        input_onehot[:, tok_to_attack, :] = input_onehot[:, tok_to_attack, :] - torch.mul(best_nabla_sampled, learning_rate)[tok_to_attack, :]\n",
    "        \n",
    "        input_onehot_softmax = torch.nn.Softmax(dim=2)(input_onehot.data)\n",
    "\n",
    "    if args.pgd_token_selection_strategy == 'argmax':\n",
    "            generated_tokenizer_idxs = input_onehot_best.argmax(1).squeeze().detach().cpu().numpy().tolist()\n",
    "\n",
    "    generated_string = tokenizer.decode(generated_tokenizer_idxs, skip_special_tokens=True)\n",
    "    (generated_prediction, _, _, generated_prediction_loss) = get_code_preds(generated_string, model_wrapper_senti.model, tokenizer, desired_target_after_attack, model_wrapper_senti.attack_loss_fn, None)\n",
    "\n",
    "    if args.verbose:\n",
    "            print(\"Best loss: {} :: prediction: {}\".format(loss_best, pred_best))\n",
    "            print(\"generated_tokenizer_idxs: {}\".format(generated_tokenizer_idxs))\n",
    "            print('Generated string:\\n{}^^\\n'.format(generated_string))\n",
    "            print('Original string:\\n{}^^\\n'.format(code_to_transform))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelWrapper(ModelWrapper):\n",
    "    \"\"\"Loads a HuggingFace ``transformers`` model and tokenizer.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                    model_encoder, \n",
    "                    model_fc, \n",
    "                    tokenizer, \n",
    "                    attack_loss_fn, \n",
    "                    debug_me=False, \n",
    "                    max_token_length=10, \n",
    "                    all_embeddings_file_name=\"all_embeddings.pkl\",\n",
    "                    layer_to_use=1, \n",
    "                    aggregation_to_use=\"last-tok\",\n",
    "                    vocab_toks_to_use=[]):\n",
    "        assert isinstance(\n",
    "            tokenizer,\n",
    "            (transformers.PreTrainedTokenizer, transformers.PreTrainedTokenizerFast),\n",
    "        ), f\"`tokenizer` must of type `transformers.PreTrainedTokenizer` or `transformers.PreTrainedTokenizerFast`, but got type {type(tokenizer)}.\"\n",
    "\n",
    "        self.model = model_encoder\n",
    "        self.model_fc = model_fc\n",
    "        self.tokenizer = tokenizer\n",
    "        self._max_length = max_token_length\n",
    "        self.attack_loss_fn = attack_loss_fn\n",
    "        self.list_emb = list() # Stores a 1D tensor of all embeddings of tokens in the vocab\n",
    "        self.list_toks = list() # Stores the tokens corresponding to the indicies in list_emb\n",
    "        self.debug_me = debug_me\n",
    "        self.all_embeddings_file_name = all_embeddings_file_name\n",
    "        self.layer_to_use = layer_to_use\n",
    "        self.aggregation_to_use = aggregation_to_use\n",
    "        self.model_device = next(self.model.parameters()).device\n",
    "        # self.get_all_embeddings(vocab_toks_to_use)\n",
    "\n",
    "\n",
    "    def get_grad_v2(self, input_code, desired_label_or_target_after_attack, current_embedding=None, input_onehot=None, print_debug=False):\n",
    "        \"\"\"Get gradient of loss with respect to input tokens.\n",
    "\n",
    "        Args:\n",
    "            input_dict (dict): contains keys 'input_ids' and 'attention_mask' needed by the model\n",
    "        Returns:\n",
    "            Dict of ids, tokens, and gradient as numpy array.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        #self.model_fc.eval()\n",
    "        if input_onehot is not None:\n",
    "            input_onehot.grad = None\n",
    "            input_onehot.requires_grad = True\n",
    "            input_onehot.retain_grad()\n",
    "\n",
    "        embedding_layer = self.model.get_input_embeddings()\n",
    "        original_state = embedding_layer.weight.requires_grad\n",
    "        embedding_layer.weight.requires_grad = True\n",
    "\n",
    "        emb_grads = []\n",
    "        if current_embedding is not None:\n",
    "            # current_embedding.requires_grad = True\n",
    "            current_embedding.retain_grad()\n",
    "\n",
    "        def output_hook(module, input, output):\n",
    "            if current_embedding is not None:\n",
    "                if not self.debug_me:\n",
    "                    output.data.copy_(current_embedding)\n",
    "                else:\n",
    "                    output.data = torch.zeros(current_embedding.shape, device=current_embedding.device)\n",
    "            \n",
    "            return output\n",
    "\n",
    "        def grad_hook(module, grad_in, grad_out):\n",
    "            emb_grads.append(grad_out[0])\n",
    "\n",
    "        emb_bck_hook = embedding_layer.register_full_backward_hook(grad_hook)\n",
    "        emb_fwd_hook_handle = embedding_layer.register_forward_hook(output_hook)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        input_dict = self.tokenizer(input_code, padding=False, return_tensors='pt', add_special_tokens=True)\n",
    "        # print(input_dict.input_ids)\n",
    "\n",
    "        prediction = self.model(input_dict.input_ids.to(self.model_device), output_hidden_states=True, return_dict=True, one_hot=input_onehot).logits.squeeze()\n",
    "        \n",
    "        loss = self.attack_loss_fn(prediction.unsqueeze(0), torch.tensor(desired_label_or_target_after_attack).unsqueeze(0))\n",
    "        \n",
    "        if print_debug:\n",
    "            print(\"Prediction: {}; Loss :: {}\".format(prediction, loss.squeeze().data.numpy().tolist()))\n",
    "        # print(\"Loss shape :: \", loss.shape)\n",
    "        loss.backward()\n",
    "\n",
    "        # grad w.r.t to word embeddings\n",
    "        # grad = emb_grads.squeeze() #.cpu().numpy()\n",
    "        grad = input_onehot.grad.squeeze()\n",
    "        \n",
    "        embeddings = embedding_layer(input_dict['input_ids'])        \n",
    "        embedding_layer.weight.requires_grad = original_state\n",
    "        \n",
    "        emb_fwd_hook_handle.remove()\n",
    "        emb_bck_hook.remove()\n",
    "        \n",
    "        output = {\"ids\": input_dict, \"gradient\": grad, \"embedding\": embeddings, \"loss\":loss.detach().cpu().numpy().tolist(), \"prediction\": prediction.detach().cpu().numpy().tolist()}\n",
    "        \n",
    "        if self.debug_me:\n",
    "            print(output['gradient'].shape)\n",
    "            print(output['embedding'].shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "config_bert = AutoConfig.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "\n",
    "model1_bert = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "model2_bert = BertForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "custom_bert = CustomBertForSequenceClassification(config_bert, config_bert.vocab_size, config_bert.hidden_size)\n",
    "custom_bert.load_state_dict(model2_bert.state_dict(), strict=False)\n",
    "custom_bert.update_weights()\n",
    "custom_bert.bert_v2.update_weights()\n",
    "custom_bert.bert_v2.embeddings_v2.update_weights()\n",
    "custom_bert.eval()\n",
    "\n",
    "vocab_tokens_to_use_bert, vocab_tokens_to_ignore_bert, vocab_tokens_not_upper_case_bert, vocab_tokens_upper_case_bert = get_vocab_tokens_to_use(tokenizer_bert)\n",
    "attack_loss_fn = nn.CrossEntropyLoss()\n",
    "model_wrapper_senti = HuggingFaceModelWrapper(model_encoder=custom_bert, \n",
    "                                        model_fc=None, \n",
    "                                        tokenizer=tokenizer_bert, \n",
    "                                        attack_loss_fn=attack_loss_fn,\n",
    "                                        max_token_length=tokenizer_bert.model_max_length,\n",
    "                                        debug_me=False,\n",
    "                                        all_embeddings_file_name=None,\n",
    "                                        layer_to_use=None,\n",
    "                                        aggregation_to_use=None,\n",
    "                                        vocab_toks_to_use=vocab_tokens_to_use_bert)\n",
    "\n",
    "if args.pgd_data_root is not None:\n",
    "    paths.DATAROOT = args.pgd_data_root\n",
    "\n",
    "pgd_attack_iters_senti = args.pgd_iter # 5\n",
    "learning_rate_pgd_senti  = args.pgd_lr # 1.5\n",
    "desired_target_after_attack_senti = int(args.pgd_desired_target) # 100.0\n",
    "# desired_target_after_attack_senti = torch.tensor(desired_target_senti, requires_grad=True).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, x in enumerate(range(len(stimset))):\n",
    "    print(\"******* code idx:: {}\".format(cnt+1))\n",
    "    current_stimid = stimset.index[x]\n",
    "    code_to_transform = stimset.loc[current_stimid]['code']\n",
    "    per_code_synthesis_senti(code_to_transform, \n",
    "                                    tokenizer_bert, \n",
    "                                    config_bert, \n",
    "                                    model_wrapper_senti, \n",
    "                                    custom_bert, \n",
    "                                    desired_target_after_attack_senti, \n",
    "                                    args, \n",
    "                                    convert_to_onehot, \n",
    "                                    get_toks_per_word,\n",
    "                                    pgd_attack_iters_senti,\n",
    "                                    learning_rate_pgd_senti,\n",
    "                                    vocab_tokens_to_use_bert,\n",
    "                                    number_to_dump,\n",
    "                                    paths, \n",
    "                                    model1_bert, model2_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4code-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31bd4b420e5dc3581e1ac5ca87b0de55aed9eadbb6f1d77fc9233ebba102472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
