{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not execute this cell if you are running this locally.\n",
    "# Instead, first run the steps in the README and then run the cells below.\n",
    "\n",
    "%%capture\n",
    "%git clone https://github.com/shashank-srikant/ai4code-tutorial\n",
    "%cd ai4code-tutorial\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_toks_per_word, get_code_preds, convert_to_onehot, get_most_sensitive_sites, get_vocab_tokens_to_use\n",
    "from gradients import get_grad\n",
    "from custom_bert import CustomBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset\n",
    "!gdown https://drive.google.com/uc?id=1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset\n",
    "!python preprocess.py\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "def get_dataset(pth, file='train', number_of_files=5, max_length=512, select_target=0):\n",
    "    codes, targets, idxs = [], [], []\n",
    "    \n",
    "    with open(os.path.join('.', pth, file+'.jsonl'), 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    \n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        if 'target' in result and 'func' in result and 'idx' in result:\n",
    "            if result['target'] == select_target and len(result['func'].split(' ')) < max_length:\n",
    "                codes.append(result['func'])\n",
    "                targets.append(result['target'])\n",
    "                idxs.append(result['idx'])\n",
    "\n",
    "    rand_idxs = random.sample(range(len(codes)), number_of_files)\n",
    "    \n",
    "    def select(li, ixs):\n",
    "        return [li[ix] for ix in ixs]\n",
    "\n",
    "    return select(codes, rand_idxs), select(targets, rand_idxs), select(idxs, rand_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(program_to_transform, \n",
    "                model,\n",
    "                tokenizer, \n",
    "                config,\n",
    "                loss_fn,\n",
    "                desired_target,\n",
    "                device,\n",
    "                results_root,\n",
    "                opti_iters,\n",
    "                learning_rate,\n",
    "                number_of_sites,\n",
    "                multinomial_samples,\n",
    "                sample_info,\n",
    "                verbose=False\n",
    "                ):\n",
    "    \n",
    "    result_dump = []\n",
    "    result_dump.insert(0, ['pgd_iters: '+str(opti_iters), \n",
    "                    'pgd_lr: '+str(learning_rate),\n",
    "                    'number of multinomial samples: '+str(multinomial_samples),\n",
    "                    'sample info '+str(sample_info),\n",
    "                    '', '', '', ''])\n",
    "\n",
    "    result_dump.insert(1, ['ID', \n",
    "                    'Program',\n",
    "                    'Fixed program'\n",
    "                    'Model output', \n",
    "                    'Best loss iters',\n",
    "                    'Processing time',\n",
    "                    ])\n",
    "    \n",
    "    df = pd.DataFrame(result_dump)\n",
    "    identifier_dir = str(opti_iters)+\"_\"+\\\n",
    "                str(learning_rate)+\"_\"+\\\n",
    "                str(multinomial_samples)+\"_\"\n",
    "    \n",
    "    pathlib.Path(os.path.join(results_root, \"results_\"+identifier_dir)).mkdir(parents=True, exist_ok=True)\n",
    "    identifier = identifier_dir\n",
    "    df.to_csv(os.path.join(results_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), index=False, header=False)\n",
    "\n",
    "    t_start = time.time()\n",
    "    \t\t\n",
    "    orig_target = 0 if desired_target == 1 else 1\n",
    "    (prediction_orig, tok_idxs, encoded_idxs, loss_orig) = get_code_preds(program_to_transform, model, tokenizer, orig_target, loss_fn, None)\n",
    "    # print(\"Original code: {}; Predicted activation: {}\\n^^^^^\\n\".format(program_to_transform, prediction))\n",
    "\n",
    "    input_onehot = convert_to_onehot(tok_idxs, vocab_size=len(tokenizer), device=device)\n",
    "    input_onehot_orig = input_onehot.detach().clone()\n",
    "\n",
    "    loss_iters, generated_tokenizer_idxs = [], None\n",
    "    input_onehot_best = None\n",
    "    loss_best = 100 #loss_prediction\n",
    "    pred_best = 0\n",
    "    \n",
    "    ## Test whether onehot preds work as expected\n",
    "    input_onehot.grad = None\n",
    "    input_onehot.requires_grad = True\n",
    "    input_onehot.retain_grad()\n",
    "    (prediction_oh, _, _, _) = get_code_preds(program_to_transform, model, tokenizer, None, None, input_onehot)\n",
    "    assert torch.equal(prediction_oh, prediction_orig)\n",
    "\n",
    "    tok_to_attack = get_most_sensitive_sites(model, program_to_transform, desired_target, input_onehot, number_of_sites, get_toks_per_word, tokenizer, loss_fn, device)\n",
    "    input_onehot.requires_grad = False\t\t\t\n",
    "\n",
    "    input_onehot_softmax = input_onehot.data.clone()\n",
    "\n",
    "    for attack_cnt in range(opti_iters):\n",
    "        loss_best_sampled = 100 #loss_prediction\n",
    "        pred_best_sampled = 0\n",
    "        best_input_onehot_sampled, best_nabla_sampled = None, None\n",
    "\n",
    "        if attack_cnt % 5 == 0:\n",
    "                flg = True\n",
    "        else:\n",
    "                flg = False\n",
    "\n",
    "        for _ in range(multinomial_samples):\n",
    "            input_onehot_softmax_ = input_onehot_softmax.data.numpy()[0,:]\n",
    "            sampled_oh_ = []\n",
    "            for tok_idx in range(input_onehot_softmax_.shape[0]):\n",
    "                if tok_idx in tok_to_attack:\n",
    "                    sampled_oh_.append(np.random.multinomial(1, input_onehot_softmax_[tok_idx]))\n",
    "                else:\n",
    "                    sampled_oh_.append(input_onehot.data[:, tok_idx, :].squeeze(0).numpy())\n",
    "            sampled_oh = np.stack(sampled_oh_)\n",
    "            input_onehot_softmax_sampled = torch.tensor(sampled_oh, requires_grad=True, dtype=torch.float, device=device)\n",
    "            grads_and_embeddings = get_grad(model, tokenizer, loss_fn, device, program_to_transform, desired_target, None, input_onehot_softmax_sampled, False)\n",
    "            if (grads_and_embeddings['loss'] < loss_best_sampled):\n",
    "                loss_best_sampled = grads_and_embeddings['loss']\n",
    "                pred_best_sampled = grads_and_embeddings['prediction']\n",
    "                best_input_onehot_sampled = input_onehot_softmax_sampled.detach().clone()\n",
    "                best_nabla_sampled = grads_and_embeddings['gradient'].detach()\n",
    "                if verbose:\n",
    "                        print(\"Loss: {}; Pred: {}\".format(loss_best_sampled, grads_and_embeddings['prediction']))\n",
    "                \n",
    "        loss_iters.append(loss_best_sampled)\n",
    "\n",
    "        if (loss_best_sampled < loss_best): # or (desired_target < 0 and loss_best_sampled > loss_best):\n",
    "            loss_best = loss_best_sampled\n",
    "            pred_best = pred_best_sampled\n",
    "            input_onehot_best = best_input_onehot_sampled.data\n",
    "            if verbose:\n",
    "                print(\"Loss: {}; Pred: {}; iter: {}\".format(loss_best, pred_best, attack_cnt))\n",
    "\n",
    "        input_onehot[:, tok_to_attack, :] = input_onehot[:, tok_to_attack, :] - torch.mul(best_nabla_sampled, learning_rate)[tok_to_attack, :]\n",
    "        \n",
    "        input_onehot_softmax = torch.nn.Softmax(dim=2)(input_onehot.data)\n",
    "\n",
    "    generated_tokenizer_idxs = input_onehot_best.argmax(1).squeeze().detach().cpu().numpy().tolist()\n",
    "    generated_string = tokenizer.decode(generated_tokenizer_idxs, skip_special_tokens=True)\n",
    "    (generated_prediction, _, _, generated_prediction_loss) = get_code_preds(generated_string, model, tokenizer, desired_target, loss_fn, None)\n",
    "\n",
    "    if verbose:\n",
    "            print(\"Best loss: {} :: prediction: {}\".format(loss_best, pred_best))\n",
    "            print(\"generated_tokenizer_idxs: {}\".format(generated_tokenizer_idxs))\n",
    "            print('Generated string:\\n{}^^\\n'.format(generated_string))\n",
    "            print('Original string:\\n{}^^\\n'.format(program_to_transform))\n",
    "    \n",
    "    t_end = time.time()\n",
    "    df = pd.DataFrame([program_to_transform, \n",
    "                        prediction_orig, \n",
    "                        loss_orig,\n",
    "                        generated_string, \n",
    "                        generated_prediction,\n",
    "                        generated_prediction_loss,\n",
    "                        loss_best,\n",
    "                        str(number_of_sites),\n",
    "                        str(len(tok_to_attack)),\n",
    "                        str(tok_to_attack), \n",
    "                        int((t_end-t_start)/60),\n",
    "                        sample_info\n",
    "                        ]).transpose()\n",
    "    \n",
    "    # with lock:\n",
    "    df.to_csv(os.path.join(results_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), mode='a', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(program_to_transform, \n",
    "                model,\n",
    "                tokenizer, \n",
    "                config,\n",
    "                loss_fn,\n",
    "                desired_target,\n",
    "                device,\n",
    "                results_root,\n",
    "                opti_iters,\n",
    "                learning_rate,\n",
    "                number_of_sites,\n",
    "                multinomial_samples,\n",
    "                sample_info,\n",
    "                verbose=False\n",
    "                ):\n",
    "    \n",
    "    result_dump = []\n",
    "    result_dump.insert(0, ['pgd_iters: '+str(opti_iters), \n",
    "                    'pgd_lr: '+str(learning_rate),\n",
    "                    'number of multinomial samples: '+str(multinomial_samples),\n",
    "                    'sample info '+str(sample_info),\n",
    "                    '', '', '', ''])\n",
    "\n",
    "    result_dump.insert(1, ['ID', \n",
    "                    'Program',\n",
    "                    'Fixed program'\n",
    "                    'Model output', \n",
    "                    'Best loss iters',\n",
    "                    'Processing time',\n",
    "                    ])\n",
    "    \n",
    "    df = pd.DataFrame(result_dump)\n",
    "    identifier_dir = str(opti_iters)+\"_\"+\\\n",
    "                str(learning_rate)+\"_\"+\\\n",
    "                str(multinomial_samples)+\"_\"\n",
    "    \n",
    "    pathlib.Path(os.path.join(results_root, \"results_\"+identifier_dir)).mkdir(parents=True, exist_ok=True)\n",
    "    identifier = identifier_dir\n",
    "    df.to_csv(os.path.join(results_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), index=False, header=False)\n",
    "\n",
    "    t_start = time.time()\n",
    "    \t\t\n",
    "    orig_target = 0 if desired_target == 1 else 1\n",
    "    (prediction_orig, tok_idxs, encoded_idxs, loss_orig) = get_code_preds(program_to_transform, model, tokenizer, orig_target, loss_fn, None)\n",
    "    # print(\"Original code: {}; Predicted activation: {}\\n^^^^^\\n\".format(program_to_transform, prediction))\n",
    "\n",
    "    input_onehot = convert_to_onehot(tok_idxs, vocab_size=len(tokenizer), device=device)\n",
    "    input_onehot_orig = input_onehot.detach().clone()\n",
    "\n",
    "    loss_iters, generated_tokenizer_idxs = [], None\n",
    "    input_onehot_best = None\n",
    "    loss_best = 100 #loss_prediction\n",
    "    pred_best = 0\n",
    "    \n",
    "    ## Test whether onehot preds work as expected\n",
    "    input_onehot.grad = None\n",
    "    input_onehot.requires_grad = True\n",
    "    input_onehot.retain_grad()\n",
    "    (prediction_oh, _, _, _) = get_code_preds(program_to_transform, model, tokenizer, None, None, input_onehot)\n",
    "    assert torch.equal(prediction_oh, prediction_orig)\n",
    "\n",
    "    tok_to_attack = get_most_sensitive_sites(model, program_to_transform, desired_target, input_onehot, number_of_sites, get_toks_per_word, tokenizer, loss_fn, device)\n",
    "    input_onehot.requires_grad = False\t\t\t\n",
    "\n",
    "    input_onehot_softmax = input_onehot.data.clone()\n",
    "\n",
    "    for attack_cnt in range(opti_iters):\n",
    "        loss_best_sampled = 100 #loss_prediction\n",
    "        pred_best_sampled = 0\n",
    "        best_input_onehot_sampled, best_nabla_sampled = None, None\n",
    "\n",
    "        if attack_cnt % 5 == 0:\n",
    "                flg = True\n",
    "        else:\n",
    "                flg = False\n",
    "\n",
    "        for _ in range(multinomial_samples):\n",
    "            input_onehot_softmax_ = input_onehot_softmax.data.numpy()[0,:]\n",
    "            sampled_oh_ = []\n",
    "            for tok_idx in range(input_onehot_softmax_.shape[0]):\n",
    "                if tok_idx in tok_to_attack:\n",
    "                    sampled_oh_.append(np.random.multinomial(1, input_onehot_softmax_[tok_idx]))\n",
    "                else:\n",
    "                    sampled_oh_.append(input_onehot.data[:, tok_idx, :].squeeze(0).numpy())\n",
    "            sampled_oh = np.stack(sampled_oh_)\n",
    "            input_onehot_softmax_sampled = torch.tensor(sampled_oh, requires_grad=True, dtype=torch.float, device=device)\n",
    "            grads_and_embeddings = get_grad(model, tokenizer, loss_fn, device, program_to_transform, desired_target, None, input_onehot_softmax_sampled, False)\n",
    "            if (grads_and_embeddings['loss'] < loss_best_sampled):\n",
    "                loss_best_sampled = grads_and_embeddings['loss']\n",
    "                pred_best_sampled = grads_and_embeddings['prediction']\n",
    "                best_input_onehot_sampled = input_onehot_softmax_sampled.detach().clone()\n",
    "                best_nabla_sampled = grads_and_embeddings['gradient'].detach()\n",
    "                if verbose:\n",
    "                        print(\"Loss: {}; Pred: {}\".format(loss_best_sampled, grads_and_embeddings['prediction']))\n",
    "                \n",
    "        loss_iters.append(loss_best_sampled)\n",
    "\n",
    "        if (loss_best_sampled < loss_best): # or (desired_target < 0 and loss_best_sampled > loss_best):\n",
    "            loss_best = loss_best_sampled\n",
    "            pred_best = pred_best_sampled\n",
    "            input_onehot_best = best_input_onehot_sampled.data\n",
    "            if verbose:\n",
    "                print(\"Loss: {}; Pred: {}; iter: {}\".format(loss_best, pred_best, attack_cnt))\n",
    "\n",
    "        input_onehot[:, tok_to_attack, :] = input_onehot[:, tok_to_attack, :] - torch.mul(best_nabla_sampled, learning_rate)[tok_to_attack, :]\n",
    "        \n",
    "        input_onehot_softmax = torch.nn.Softmax(dim=2)(input_onehot.data)\n",
    "\n",
    "    generated_tokenizer_idxs = input_onehot_best.argmax(1).squeeze().detach().cpu().numpy().tolist()\n",
    "    generated_string = tokenizer.decode(generated_tokenizer_idxs, skip_special_tokens=True)\n",
    "    (generated_prediction, _, _, generated_prediction_loss) = get_code_preds(generated_string, model, tokenizer, desired_target, loss_fn, None)\n",
    "\n",
    "    if verbose:\n",
    "            print(\"Best loss: {} :: prediction: {}\".format(loss_best, pred_best))\n",
    "            print(\"generated_tokenizer_idxs: {}\".format(generated_tokenizer_idxs))\n",
    "            print('Generated string:\\n{}^^\\n'.format(generated_string))\n",
    "            print('Original string:\\n{}^^\\n'.format(program_to_transform))\n",
    "    \n",
    "    t_end = time.time()\n",
    "    df = pd.DataFrame([program_to_transform, \n",
    "                        prediction_orig, \n",
    "                        loss_orig,\n",
    "                        generated_string, \n",
    "                        generated_prediction,\n",
    "                        generated_prediction_loss,\n",
    "                        loss_best,\n",
    "                        str(number_of_sites),\n",
    "                        str(len(tok_to_attack)),\n",
    "                        str(tok_to_attack), \n",
    "                        int((t_end-t_start)/60),\n",
    "                        sample_info\n",
    "                        ]).transpose()\n",
    "    \n",
    "    # with lock:\n",
    "    df.to_csv(os.path.join(results_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), mode='a', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "def get_models(model_name):\n",
    "    model_and_tokenizer = []\n",
    "    for m in model_name:\n",
    "        if m == 'codeberta-finetuned':\n",
    "            tokenizer_bert = AutoTokenizer.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            config_bert = AutoConfig.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "\n",
    "            model1_bert = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            model2_bert = BertForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            custom_bert = CustomBertForSequenceClassification(config_bert, config_bert.vocab_size, config_bert.hidden_size)\n",
    "            custom_bert.load_state_dict(model2_bert.state_dict(), strict=False)\n",
    "            custom_bert.update_weights()\n",
    "            custom_bert.bert_v2.update_weights()\n",
    "            custom_bert.bert_v2.embeddings_v2.update_weights()\n",
    "            custom_bert.eval()\n",
    "\n",
    "            # vocab_tokens_to_use_bert, vocab_tokens_to_ignore_bert, vocab_tokens_not_upper_case_bert, vocab_tokens_upper_case_bert = get_vocab_tokens_to_use(tokenizer_bert)\n",
    "            \n",
    "            model_and_tokenizer.append((custom_bert, tokenizer_bert, config_bert))\n",
    "    \n",
    "    return model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_names = ['codeberta-finetuned'] # ['codeberta-base-mlm', 'plbart']\n",
    "iters = 5\n",
    "learning_rate  = 0.1\n",
    "desired_target = 1 # 100.0\n",
    "multinomial_samples = 1\n",
    "number_of_codes_to_optimize = 1\n",
    "number_of_sites =1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "verbose = True\n",
    "\n",
    "expt_dir = 'results'\n",
    "data_dir = 'dataset'\n",
    "\n",
    "models = get_models(model_names)\n",
    "all_results = {}    \n",
    "for (model, tokenizer, config), model_name in zip(models, model_names):\n",
    "    codes, targets, sample_info = get_dataset(pth=data_dir, file='valid', number_of_files=number_of_codes_to_optimize, max_length=100)\n",
    "    for c, t, s in zip(codes, targets, sample_info):\n",
    "        desired_target = 1 if t == 0 else 0\n",
    "        optimize(c, \n",
    "                model, \n",
    "                tokenizer, \n",
    "                config, \n",
    "                loss_fn, \n",
    "                desired_target, \n",
    "                device, \n",
    "                expt_dir, \n",
    "                iters, \n",
    "                learning_rate, \n",
    "                number_of_sites, \n",
    "                multinomial_samples, \n",
    "                s, \n",
    "                verbose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4code-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11 (default, Aug  3 2021, 15:09:35) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31bd4b420e5dc3581e1ac5ca87b0de55aed9eadbb6f1d77fc9233ebba102472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
