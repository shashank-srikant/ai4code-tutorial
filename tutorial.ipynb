{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not execute this cell if you are running this locally.\n",
    "# Instead, first run the steps in the README and then run the cells below.\n",
    "\n",
    "%%capture\n",
    "%git clone https://github.com/shashank-srikant/ai4code-tutorial\n",
    "%cd ai4code-tutorial\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_toks_per_word, get_code_preds, convert_to_onehot, get_most_sensitive_sites\n",
    "from custom_bert import CustomBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset\n",
    "!gdown https://drive.google.com/uc?id=1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset\n",
    "!python preprocess.py\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_dataset(pth, file='train', number_of_files=5):\n",
    "    codes, targets, idxs = [], [], []\n",
    "    \n",
    "    with open(os.path.join('.', pth, file+'.jsonl'), 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    \n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        if 'target' in result and 'func' in result and 'idx' in result:\n",
    "            codes.append(result['func'])\n",
    "            targets.append(result['target'])\n",
    "            idxs.append(result['idx'])\n",
    "        \n",
    "    return codes, targets, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(model, tokenizer, loss_fn, model_device, input_program, desired_label_or_target_after_attack, current_embedding=None, input_onehot=None, print_debug=False):\n",
    "    \"\"\"Get gradient of loss with respect to input tokens.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): contains keys 'input_ids' and 'attention_mask' needed by the model\n",
    "    Returns:\n",
    "        Dict of ids, tokens, and gradient as numpy array.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if input_onehot is not None:\n",
    "        input_onehot.grad = None\n",
    "        input_onehot.requires_grad = True\n",
    "        input_onehot.retain_grad()\n",
    "\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    original_state = embedding_layer.weight.requires_grad\n",
    "    embedding_layer.weight.requires_grad = True\n",
    "\n",
    "    emb_grads = []\n",
    "    if current_embedding is not None:\n",
    "        # current_embedding.requires_grad = True\n",
    "        current_embedding.retain_grad()\n",
    "\n",
    "    def output_hook(module, input, output):\n",
    "        if current_embedding is not None:\n",
    "            if not print_debug:\n",
    "                output.data.copy_(current_embedding)\n",
    "            else:\n",
    "                output.data = torch.zeros(current_embedding.shape, device=current_embedding.device)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def grad_hook(module, grad_in, grad_out):\n",
    "        emb_grads.append(grad_out[0])\n",
    "\n",
    "    emb_bck_hook = embedding_layer.register_full_backward_hook(grad_hook)\n",
    "    emb_fwd_hook_handle = embedding_layer.register_forward_hook(output_hook)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    input_dict = tokenizer(input_program, padding=False, return_tensors='pt', add_special_tokens=True)\n",
    "    # print(input_dict.input_ids)\n",
    "\n",
    "    prediction = model(input_dict.input_ids.to(model_device), output_hidden_states=True, return_dict=True, one_hot=input_onehot).logits.squeeze()\n",
    "    \n",
    "    loss = loss_fn(prediction.unsqueeze(0), torch.tensor(desired_label_or_target_after_attack).unsqueeze(0))\n",
    "    \n",
    "    if print_debug:\n",
    "        print(\"Prediction: {}; Loss :: {}\".format(prediction, loss.squeeze().data.numpy().tolist()))\n",
    "    # print(\"Loss shape :: \", loss.shape)\n",
    "    loss.backward()\n",
    "\n",
    "    # grad w.r.t to word embeddings\n",
    "    # grad = emb_grads.squeeze() #.cpu().numpy()\n",
    "    grad = input_onehot.grad.squeeze()\n",
    "    \n",
    "    embeddings = embedding_layer(input_dict['input_ids'])        \n",
    "    embedding_layer.weight.requires_grad = original_state\n",
    "    \n",
    "    emb_fwd_hook_handle.remove()\n",
    "    emb_bck_hook.remove()\n",
    "    \n",
    "    output = {\"ids\": input_dict, \"gradient\": grad, \"embedding\": embeddings, \"loss\":loss.detach().cpu().numpy().tolist(), \"prediction\": prediction.detach().cpu().numpy().tolist()}\n",
    "    \n",
    "    if print_debug:\n",
    "        print(output['gradient'].shape)\n",
    "        print(output['embedding'].shape)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(program_to_transform, \n",
    "                model,\n",
    "                tokenizer, \n",
    "                config,\n",
    "                loss_fn,\n",
    "                desired_target,\n",
    "                device,\n",
    "                data_root,\n",
    "                opti_iters,\n",
    "                learning_rate,\n",
    "                number_of_sites,\n",
    "                multinomial_samples,\n",
    "                sample_info,\n",
    "                verbose=False\n",
    "                ):\n",
    "    \n",
    "    result_dump = []\n",
    "    result_dump.insert(0, ['pgd_iters: '+str(opti_iters), \n",
    "                    'pgd_lr: '+str(learning_rate),\n",
    "                    'number of multinomial samples: '+str(multinomial_samples),\n",
    "                    'sample info '+str(sample_info),\n",
    "                    '', '', '', ''])\n",
    "\n",
    "    result_dump.insert(1, ['ID', \n",
    "                    'Program',\n",
    "                    'Fixed program'\n",
    "                    'Model output', \n",
    "                    'Best loss iters',\n",
    "                    'Processing time',\n",
    "                    ])\n",
    "    \n",
    "    df = pd.DataFrame(result_dump)\n",
    "    identifier_dir = str(opti_iters)+\"_\"+\\\n",
    "                str(learning_rate)+\"_\"+\\\n",
    "                str(multinomial_samples)+\"_\"\n",
    "    \n",
    "    pathlib.Path(os.path.join(data_root, \"results_\"+identifier_dir)).mkdir(parents=True, exist_ok=True)\n",
    "    identifier = identifier_dir\n",
    "    df.to_csv(os.path.join(data_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), index=False, header=False)\n",
    "\n",
    "    t_start = time.time()\n",
    "    \t\t\n",
    "    (prediction_orig, tok_idxs, encoded_idxs, loss_orig) = get_code_preds(program_to_transform, model, tokenizer, loss_fn, None)\n",
    "    # print(\"Original code: {}; Predicted activation: {}\\n^^^^^\\n\".format(program_to_transform, prediction))\n",
    "\n",
    "    input_onehot = convert_to_onehot(tok_idxs, vocab_size=len(tokenizer), device=device)\n",
    "    input_onehot_orig = input_onehot.detach().clone()\n",
    "\n",
    "    loss_iters, generated_tokenizer_idxs = [], None\n",
    "    input_onehot_best = None\n",
    "    loss_best = 100 #loss_prediction\n",
    "    pred_best = 0\n",
    "    \n",
    "    ## Test whether onehot preds work as expected\n",
    "    input_onehot.grad = None\n",
    "    input_onehot.requires_grad = True\n",
    "    input_onehot.retain_grad()\n",
    "    (prediction_oh, _, _, _) = get_code_preds(program_to_transform, model, tokenizer, None, None, input_onehot)\n",
    "    assert torch.equal(prediction_oh, prediction_orig)\n",
    "\n",
    "    tok_to_attack = get_most_sensitive_sites(model, program_to_transform, desired_target, input_onehot, number_of_sites, get_toks_per_word, tokenizer)\n",
    "    input_onehot.requires_grad = False\t\t\t\n",
    "\n",
    "    input_onehot_softmax = input_onehot.data.clone()\n",
    "\n",
    "    for attack_cnt in range(opti_iters):\n",
    "        loss_best_sampled = 100 #loss_prediction\n",
    "        pred_best_sampled = 0\n",
    "        best_input_onehot_sampled, best_nabla_sampled = None, None\n",
    "\n",
    "        if attack_cnt % 5 == 0:\n",
    "                flg = True\n",
    "        else:\n",
    "                flg = False\n",
    "\n",
    "        for _ in range(multinomial_samples):\n",
    "            input_onehot_softmax_ = input_onehot_softmax.data.numpy()[0,:]\n",
    "            sampled_oh_ = []\n",
    "            for tok_idx in range(input_onehot_softmax_.shape[0]):\n",
    "                if tok_idx in tok_to_attack:\n",
    "                    sampled_oh_.append(np.random.multinomial(1, input_onehot_softmax_[tok_idx]))\n",
    "                else:\n",
    "                    sampled_oh_.append(input_onehot.data[:, tok_idx, :].squeeze(0).numpy())\n",
    "            sampled_oh = np.stack(sampled_oh_)\n",
    "            input_onehot_softmax_sampled = torch.tensor(sampled_oh, requires_grad=True, dtype=torch.double, device=args.device)\n",
    "            grads_and_embeddings = get_grad(program_to_transform, desired_target, None, input_onehot_softmax_sampled, False)\n",
    "            if (grads_and_embeddings['loss'] < loss_best_sampled):\n",
    "                loss_best_sampled = grads_and_embeddings['loss']\n",
    "                pred_best_sampled = grads_and_embeddings['prediction']\n",
    "                best_input_onehot_sampled = input_onehot_softmax_sampled.detach().clone()\n",
    "                best_nabla_sampled = grads_and_embeddings['gradient'].detach()\n",
    "                if verbose:\n",
    "                        print(\"Loss: {}; Pred: {}\".format(loss_best_sampled, grads_and_embeddings['prediction']))\n",
    "                \n",
    "        loss_iters.append(loss_best_sampled)\n",
    "\n",
    "        if (loss_best_sampled < loss_best): # or (desired_target < 0 and loss_best_sampled > loss_best):\n",
    "            loss_best = loss_best_sampled\n",
    "            pred_best = pred_best_sampled\n",
    "            input_onehot_best = best_input_onehot_sampled.data\n",
    "            if verbose:\n",
    "                print(\"Loss: {}; Pred: {}; iter: {}\".format(loss_best, pred_best, attack_cnt))\n",
    "\n",
    "        input_onehot[:, tok_to_attack, :] = input_onehot[:, tok_to_attack, :] - torch.mul(best_nabla_sampled, learning_rate)[tok_to_attack, :]\n",
    "        \n",
    "        input_onehot_softmax = torch.nn.Softmax(dim=2)(input_onehot.data)\n",
    "\n",
    "    generated_tokenizer_idxs = input_onehot_best.argmax(1).squeeze().detach().cpu().numpy().tolist()\n",
    "    generated_string = tokenizer.decode(generated_tokenizer_idxs, skip_special_tokens=True)\n",
    "    (generated_prediction, _, _, generated_prediction_loss) = get_code_preds(generated_string, model, tokenizer, desired_target, loss_fn, None)\n",
    "\n",
    "    if verbose:\n",
    "            print(\"Best loss: {} :: prediction: {}\".format(loss_best, pred_best))\n",
    "            print(\"generated_tokenizer_idxs: {}\".format(generated_tokenizer_idxs))\n",
    "            print('Generated string:\\n{}^^\\n'.format(generated_string))\n",
    "            print('Original string:\\n{}^^\\n'.format(program_to_transform))\n",
    "    \n",
    "    t_end = time.time()\n",
    "    df = pd.DataFrame([program_to_transform, \n",
    "                        prediction_orig, \n",
    "                        loss_orig,\n",
    "                        generated_string, \n",
    "                        generated_prediction,\n",
    "                        generated_prediction_loss,\n",
    "                        loss_best,\n",
    "                        str(number_of_sites),\n",
    "                        str(len(tok_to_attack)),\n",
    "                        str(tok_to_attack), \n",
    "                        int((t_end-t_start)/60),\n",
    "                        sample_info\n",
    "                        ]).transpose()\n",
    "    \n",
    "    # with lock:\n",
    "    df.to_csv(os.path.join(data_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "def get_models(model_name):\n",
    "    model_and_tokenizer = []\n",
    "    for m in model_name:\n",
    "        if m == 'codeberta-finetuned':\n",
    "            tokenizer_bert = AutoTokenizer.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            config_bert = AutoConfig.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "\n",
    "            model1_bert = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            model2_bert = BertForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            custom_bert = CustomBertForSequenceClassification(config_bert, config_bert.vocab_size, config_bert.hidden_size)\n",
    "            custom_bert.load_state_dict(model2_bert.state_dict(), strict=False)\n",
    "            custom_bert.update_weights()\n",
    "            custom_bert.bert_v2.update_weights()\n",
    "            custom_bert.bert_v2.embeddings_v2.update_weights()\n",
    "            custom_bert.eval()\n",
    "\n",
    "            vocab_tokens_to_use_bert, vocab_tokens_to_ignore_bert, vocab_tokens_not_upper_case_bert, vocab_tokens_upper_case_bert = get_vocab_tokens_to_use(tokenizer_bert)\n",
    "            \n",
    "            model_and_tokenizer.append((custom_bert, tokenizer_bert, config_bert))\n",
    "    \n",
    "    return model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_names = 'codeberta-finetuned' # ['codeberta-base-mlm', 'plbart']\n",
    "iters = 5\n",
    "learning_rate  = 0.1\n",
    "desired_target = 1 # 100.0\n",
    "multinomial_samples = 1\n",
    "number_of_codes_to_optimize = 1\n",
    "number_of_sites =1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "verbose = True\n",
    "\n",
    "expt_dir = './'\n",
    "data_dir = './'\n",
    "\n",
    "models = get_models(model_names)\n",
    "all_results = {}    \n",
    "for (model, tokenizer, config), model_name in zip(models, model_names):\n",
    "    codes, fixes, sample_info = get_dataset(pth=data_dir, number_of_files=number_of_codes_to_optimize)\n",
    "    for c, f, s in zip(codes, fixes, sample_info):\n",
    "        optimize(c, \n",
    "                model, \n",
    "                tokenizer, \n",
    "                config, \n",
    "                loss_fn, \n",
    "                desired_target, \n",
    "                device, \n",
    "                expt_dir, \n",
    "                iters, \n",
    "                learning_rate, \n",
    "                number_of_sites, \n",
    "                multinomial_samples, \n",
    "                s, \n",
    "                verbose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4code-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31bd4b420e5dc3581e1ac5ca87b0de55aed9eadbb6f1d77fc9233ebba102472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
