{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI4Code tutorial\n",
    "\n",
    "This is meant to be run on Google Colab. Please read the repo's README for instructions on how to run this on your local machine. This has been tested on Ubuntu 20.\n",
    "\n",
    "## Big picture\n",
    "\n",
    "Our goal is to _generate_ code to elicit responses from some target function.  \n",
    "In the case of this tutorial, the target function is brain responses.  \n",
    "\n",
    "The key to abstract away something like brain responses is to learn a mapping model $\\theta_\\text{map}$ which maps code representations produced by a code language model ($\\theta_\\text{LM}$) to brain responses recorded from MRI machines ($y$).\n",
    "\n",
    "_Generating_ code here is to start with some code and modify them in a way such that the modified code produces a desired target response ($y_\\text{target}$) from $\\theta_\\text{map}$.\n",
    "\n",
    "In order to get predictions from $\\theta_\\text{map}$ to look more like $y_\\text{target}$, we need to modify an input program in measured ways which ensures this. The _measured ways_ is accomplished by changing program tokens in the direction of the gradient which optimizes $y$ to look like $y_\\text{target}$.\n",
    "\n",
    "Note - The _modification_ that we perform here is gradient-based.  \n",
    "The method can be easily extended to modify typical decoders of typical language models as well to get it to _generate_ code which satisfies the target objective.\n",
    "\n",
    "## Code layout\n",
    "\n",
    "There are three important pieces in here\n",
    "- `optimize()`, present in this notebook\n",
    "- `get_grad()` in `utils.py`\n",
    "- `CustomBertEmbeddings` in `custom_bert.py`\n",
    "\n",
    "\n",
    "`optimize` is the core function which calculates gradients with respect to the input tokens, runs multionmial sampling to find the best token in an iteration, and updates tokens with the best found token.\n",
    "\n",
    "`get_grad` calculates gradients for a given goal\n",
    "\n",
    "`customBERT` allows one-hot encoded inputs to be passed into BERT's forward method.\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "The algorithm we employ to make our edits is the following. This notebook (almost) faithfully implements this algorithm. Cells have been annotated to let you know which parts of the algorithm they correspond to.\n",
    "\n",
    "![](./algo.png?_ipython_update=2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/shashank-srikant/ai4code-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ai4code-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_toks_per_word, get_code_preds, convert_to_onehot, get_most_sensitive_sites, get_vocab_tokens_to_use\n",
    "from gradients import get_grad\n",
    "from custom_bert import CustomBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset\n",
    "!gdown https://drive.google.com/uc?id=1x6hoF7G-tSYxg8AFybggypLZgMGDNHfF\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd dataset\n",
    "!python preprocess.py\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use a surrogate dataset to replicate the effect of changing brain responses.  \n",
    "We do not deal with actual brain data in this tutorial.  \n",
    "\n",
    "We essentially need a $\\theta_\\text{map}$ which predicts a target value.  \n",
    "In our case, we look at the task of defect detection from CodeXGlue.  \n",
    "The defect detection task takes a program as an input and produces a 0 or 1 to signify whether it has a vulnerability-like defect in it.\n",
    "\n",
    "A $\\theta_\\text{map}$ which predicts brain responses will be similar. Instead of predicting a 0 or 1 category, it will predict a real value in some range [min, max]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def get_dataset(pth, file='train', number_of_files=5, max_length=512, select_target=0):\n",
    "    codes, targets, idxs = [], [], []\n",
    "    \n",
    "    with open(os.path.join('.', pth, file+'.jsonl'), 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    \n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        if 'target' in result and 'func' in result and 'idx' in result:\n",
    "            if result['target'] == select_target and len(result['func'].split(' ')) < max_length:\n",
    "                codes.append(result['func'])\n",
    "                targets.append(result['target'])\n",
    "                idxs.append(result['idx'])\n",
    "\n",
    "    rand_idxs = random.sample(range(len(codes)), number_of_files)\n",
    "    \n",
    "    def select(li, ixs):\n",
    "        return [li[ix] for ix in ixs]\n",
    "\n",
    "    return select(codes, rand_idxs), select(targets, rand_idxs), select(idxs, rand_idxs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize\n",
    "\n",
    "This is the workhorse which implements steps 3 through 20 in the main algorithm.\n",
    "It returns a goal-targeted _modified_ program\n",
    "\n",
    "![](./algo.png?_ipython_update=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(program_to_transform, \n",
    "                model,\n",
    "                tokenizer, \n",
    "                config,\n",
    "                loss_fn,\n",
    "                desired_target,\n",
    "                device,\n",
    "                results_root,\n",
    "                opti_iters,\n",
    "                learning_rate,\n",
    "                number_of_sites,\n",
    "                multinomial_samples,\n",
    "                sample_info,\n",
    "                verbose=False\n",
    "                ):\n",
    "    \n",
    "    result_dump = []\n",
    "    result_dump.insert(0, ['pgd_iters: '+str(opti_iters), \n",
    "                    'pgd_lr: '+str(learning_rate),\n",
    "                    'number of multinomial samples: '+str(multinomial_samples),\n",
    "                    'sample info '+str(sample_info),\n",
    "                    '', '', '', ''])\n",
    "\n",
    "    result_dump.insert(1, ['ID', \n",
    "                    'Program',\n",
    "                    'Fixed program'\n",
    "                    'Model output', \n",
    "                    'Best loss iters',\n",
    "                    'Processing time',\n",
    "                    ])\n",
    "    \n",
    "    df = pd.DataFrame(result_dump)\n",
    "    identifier_dir = str(opti_iters)+\"_\"+\\\n",
    "                str(learning_rate)+\"_\"+\\\n",
    "                str(multinomial_samples)+\"_\"\n",
    "    \n",
    "    pathlib.Path(os.path.join(results_root, \"results_\"+identifier_dir)).mkdir(parents=True, exist_ok=True)\n",
    "    identifier = identifier_dir\n",
    "    df.to_csv(os.path.join(results_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), index=False, header=False)\n",
    "\n",
    "    t_start = time.time()\n",
    "    \t\t\n",
    "    orig_target = 0 if desired_target == 1 else 1\n",
    "    (prediction_orig, tok_idxs, encoded_idxs, loss_orig) = get_code_preds(program_to_transform, model, tokenizer, orig_target, loss_fn, None)\n",
    "    # print(\"Original code: {}; Predicted activation: {}\\n^^^^^\\n\".format(program_to_transform, prediction))\n",
    "\n",
    "    input_onehot = convert_to_onehot(tok_idxs, vocab_size=len(tokenizer), device=device)\n",
    "    input_onehot_orig = input_onehot.detach().clone()\n",
    "\n",
    "    loss_iters, generated_tokenizer_idxs = [], None\n",
    "    input_onehot_best = None\n",
    "    loss_best = 100 #loss_prediction\n",
    "    pred_best = 0\n",
    "    \n",
    "    ## Test whether onehot preds work as expected\n",
    "    input_onehot.grad = None\n",
    "    input_onehot.requires_grad = True\n",
    "    input_onehot.retain_grad()\n",
    "    (prediction_oh, _, _, _) = get_code_preds(program_to_transform, model, tokenizer, None, None, input_onehot)\n",
    "    assert torch.equal(prediction_oh, prediction_orig)\n",
    "\n",
    "    tok_to_attack = get_most_sensitive_sites(model, program_to_transform, desired_target, input_onehot, number_of_sites, get_toks_per_word, tokenizer, loss_fn, device)\n",
    "    input_onehot.requires_grad = False\t\t\t\n",
    "\n",
    "    input_onehot_softmax = input_onehot.data.clone()\n",
    "\n",
    "    for attack_cnt in range(opti_iters):\n",
    "        loss_best_sampled = 100 #loss_prediction\n",
    "        pred_best_sampled = 0\n",
    "        best_input_onehot_sampled, best_nabla_sampled = None, None\n",
    "\n",
    "        if attack_cnt % 5 == 0:\n",
    "                flg = True\n",
    "        else:\n",
    "                flg = False\n",
    "\n",
    "        for _ in range(multinomial_samples):\n",
    "            # Step 11 of the algorithm\n",
    "            input_onehot_softmax_ = input_onehot_softmax.data.numpy()[0,:]\n",
    "            sampled_oh_ = []\n",
    "            for tok_idx in range(input_onehot_softmax_.shape[0]):\n",
    "                if tok_idx in tok_to_attack:\n",
    "                    sampled_oh_.append(np.random.multinomial(1, input_onehot_softmax_[tok_idx]))\n",
    "                else:\n",
    "                    sampled_oh_.append(input_onehot.data[:, tok_idx, :].squeeze(0).numpy())\n",
    "            sampled_oh = np.stack(sampled_oh_)\n",
    "            \n",
    "            # Step 12 of the algorithm\n",
    "            input_onehot_softmax_sampled = torch.tensor(sampled_oh, requires_grad=True, dtype=torch.float, device=device)\n",
    "            \n",
    "            # Step 17 of the algorithm\n",
    "            grads_and_embeddings = get_grad(model, tokenizer, loss_fn, device, program_to_transform, desired_target, None, input_onehot_softmax_sampled, False)\n",
    "            \n",
    "            if (grads_and_embeddings['loss'] < loss_best_sampled):\n",
    "                loss_best_sampled = grads_and_embeddings['loss']\n",
    "                pred_best_sampled = grads_and_embeddings['prediction']\n",
    "                best_input_onehot_sampled = input_onehot_softmax_sampled.detach().clone()\n",
    "                best_nabla_sampled = grads_and_embeddings['gradient'].detach()\n",
    "                if verbose:\n",
    "                        print(\"Loss: {}; Pred: {}\".format(loss_best_sampled, grads_and_embeddings['prediction']))\n",
    "                \n",
    "        loss_iters.append(loss_best_sampled)\n",
    "\n",
    "        if (loss_best_sampled < loss_best): # or (desired_target < 0 and loss_best_sampled > loss_best):\n",
    "            loss_best = loss_best_sampled\n",
    "            pred_best = pred_best_sampled\n",
    "            input_onehot_best = best_input_onehot_sampled.data\n",
    "            if verbose:\n",
    "                print(\"Loss: {}; Pred: {}; iter: {}\".format(loss_best, pred_best, attack_cnt))\n",
    "\n",
    "        # Step 18 of the algorithm\n",
    "        input_onehot[:, tok_to_attack, :] = input_onehot[:, tok_to_attack, :] - torch.mul(best_nabla_sampled, learning_rate)[tok_to_attack, :]\n",
    "        \n",
    "        input_onehot_softmax = torch.nn.Softmax(dim=2)(input_onehot.data)\n",
    "\n",
    "    generated_tokenizer_idxs = input_onehot_best.argmax(1).squeeze().detach().cpu().numpy().tolist()\n",
    "    generated_string = tokenizer.decode(generated_tokenizer_idxs, skip_special_tokens=True)\n",
    "    (generated_prediction, _, _, generated_prediction_loss) = get_code_preds(generated_string, model, tokenizer, desired_target, loss_fn, None)\n",
    "\n",
    "    if verbose:\n",
    "            print(\"Best loss: {} :: prediction: {}\".format(loss_best, pred_best))\n",
    "            # print(\"generated_tokenizer_idxs: {}\".format(generated_tokenizer_idxs))\n",
    "            print('Generated program:\\n{}^^\\n'.format(generated_string))\n",
    "            print('Original program:\\n{}^^\\n'.format(program_to_transform))\n",
    "    \n",
    "    t_end = time.time()\n",
    "    df = pd.DataFrame([program_to_transform, \n",
    "                        prediction_orig, \n",
    "                        loss_orig,\n",
    "                        generated_string, \n",
    "                        generated_prediction,\n",
    "                        generated_prediction_loss,\n",
    "                        loss_best,\n",
    "                        str(number_of_sites),\n",
    "                        str(len(tok_to_attack)),\n",
    "                        str(tok_to_attack), \n",
    "                        int((t_end-t_start)/60),\n",
    "                        sample_info\n",
    "                        ]).transpose()\n",
    "    \n",
    "    # with lock:\n",
    "    df.to_csv(os.path.join(results_root, \"results_\"+identifier_dir, 'results_{}.csv'.format(identifier)), mode='a', index=False, header=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a model\n",
    "\n",
    "We work with a fine-tuned codeBERT model.  \n",
    "Such fine-tuned models have both $\\theta_\\text{LLM}$ and $\\theta_\\text{map}$ bundled into one unit.\n",
    "\n",
    "The modification we need to make here is to allow codeBERT to accept one-hot vectors instead of token-IDs. This is because the original embedding layer exposed by PyTorch does not allow gradients to propagate through it.\n",
    "\n",
    "**Ideas to extend**:\n",
    "- Implement separate $\\theta_\\text{LLM}$ and $\\theta_\\text{map}$\n",
    "- Extend another LLM like codeT5 or GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "def get_models(model_name):\n",
    "    model_and_tokenizer = []\n",
    "    for m in model_name:\n",
    "        if m == 'codeberta-finetuned':\n",
    "            tokenizer_bert = AutoTokenizer.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            config_bert = AutoConfig.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "\n",
    "            model1_bert = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            model2_bert = BertForSequenceClassification.from_pretrained(\"mrm8488/codebert-base-finetuned-detect-insecure-code\")\n",
    "            custom_bert = CustomBertForSequenceClassification(config_bert, config_bert.vocab_size, config_bert.hidden_size)\n",
    "            custom_bert.load_state_dict(model2_bert.state_dict(), strict=False)\n",
    "            custom_bert.update_weights()\n",
    "            custom_bert.bert_v2.update_weights()\n",
    "            custom_bert.bert_v2.embeddings_v2.update_weights()\n",
    "            custom_bert.eval()\n",
    "\n",
    "            # vocab_tokens_to_use_bert, vocab_tokens_to_ignore_bert, vocab_tokens_not_upper_case_bert, vocab_tokens_upper_case_bert = get_vocab_tokens_to_use(tokenizer_bert)\n",
    "            \n",
    "            model_and_tokenizer.append((custom_bert, tokenizer_bert, config_bert))\n",
    "    \n",
    "    return model_and_tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method\n",
    "\n",
    "This cell loads the dataset and calls the optimize method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_names = ['codeberta-finetuned'] # ['codeberta-base-mlm', 'plbart']\n",
    "iters = 5\n",
    "learning_rate  = 0.1\n",
    "desired_target = 1 # 100.0\n",
    "multinomial_samples = 1\n",
    "number_of_codes_to_optimize = 1\n",
    "number_of_sites =1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "verbose = True\n",
    "\n",
    "expt_dir = 'results'\n",
    "data_dir = 'dataset'\n",
    "\n",
    "models = get_models(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, targets, sample_info = get_dataset(pth=data_dir, file='valid', number_of_files=number_of_codes_to_optimize, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = {}    \n",
    "for (model, tokenizer, config), model_name in zip(models, model_names):\n",
    "    for c, t, s in zip(codes, targets, sample_info):\n",
    "        desired_target = 1 if t == 0 else 0\n",
    "        optimize(c, \n",
    "                model, \n",
    "                tokenizer, \n",
    "                config, \n",
    "                loss_fn, \n",
    "                desired_target, \n",
    "                device, \n",
    "                expt_dir, \n",
    "                iters, \n",
    "                learning_rate, \n",
    "                number_of_sites, \n",
    "                multinomial_samples, \n",
    "                s, \n",
    "                verbose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4code-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31bd4b420e5dc3581e1ac5ca87b0de55aed9eadbb6f1d77fc9233ebba102472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
